{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### This notebook is modified from <a href=\"https://www.kaggle.com/code/pjmathematician/pii-eda-presidio-baseline\">PII EDA Presidio Baseline</a> and <a href=\"https://www.kaggle.com/code/yunsuxiaozi/pii-detect-study-notebook\">PII detect study notebook</a>. "]},{"cell_type":"markdown","metadata":{},"source":["## Modifications \n","\n","#### I add my own address_recognizer and email_recognizer, URL_recognizer, and add a black list to filter potential public urls and date checker to filter noisy phone numbers. I also added Chinese note for my modifications."]},{"cell_type":"markdown","metadata":{},"source":["### Install presidio"]},{"cell_type":"code","execution_count":4,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-20T07:40:57.487624Z","iopub.status.busy":"2024-01-20T07:40:57.486577Z","iopub.status.idle":"2024-01-20T07:41:13.824483Z","shell.execute_reply":"2024-01-20T07:41:13.823011Z","shell.execute_reply.started":"2024-01-20T07:40:57.48757Z"},"trusted":true},"outputs":[],"source":["#安装python库 presidio_analyzer 不从python库里下载,而是从给定的链接处下载,更新到最新版本,并减少输出信息.\n","!pip install -U -q presidio_analyzer --no-index --find-links=/Users/0ne/Programming/Kaggle/PIIDetect/data/presidio"]},{"cell_type":"markdown","metadata":{},"source":["### Import  necessary libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:13.826839Z","iopub.status.busy":"2024-01-20T07:41:13.826484Z","iopub.status.idle":"2024-01-20T07:41:18.852087Z","shell.execute_reply":"2024-01-20T07:41:18.850875Z","shell.execute_reply.started":"2024-01-20T07:41:13.826808Z"},"trusted":true},"outputs":[],"source":["import json\n","import pandas as pd\n","\n","# Presidio\n","from presidio_analyzer import AnalyzerEngine\n","from presidio_analyzer.nlp_engine import NlpEngineProvider\n","from tqdm import tqdm\n","from typing import List\n","import pprint\n","import re\n","\n","from presidio_analyzer import (\n","    AnalyzerEngine,\n","    PatternRecognizer,\n","    EntityRecognizer,\n","    Pattern,\n","    RecognizerResult,\n",")\n","from presidio_analyzer.recognizer_registry import RecognizerRegistry\n","from presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\n","from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n","\n","from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n","from presidio_analyzer.predefined_recognizers import PhoneRecognizer\n","from dateutil import parser"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\n","  for i\n","  in range(1,\n","           5)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Import dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:18.853694Z","iopub.status.busy":"2024-01-20T07:41:18.853153Z","iopub.status.idle":"2024-01-20T07:41:21.255201Z","shell.execute_reply":"2024-01-20T07:41:21.254008Z","shell.execute_reply.started":"2024-01-20T07:41:18.853647Z"},"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/pii-detection-removal-from-educational-data/train.json'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/pii-detection-removal-from-educational-data/train.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen(train_df):\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,train_df[0].keys():\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n","File \u001b[0;32m/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/pii-detection-removal-from-educational-data/train.json'"]}],"source":["train_df = json.load(\n","    open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\")\n",")\n","print(f\"len(train_df):{len(train_df)},train_df[0].keys():{train_df[0].keys()}\")\n","print(\"-\" * 50)\n","labels = set()\n","for i in range(len(train_df)):\n","    labels.update(train_df[i][\"labels\"])\n","print(f\"labels:{labels}\")\n","test_df = json.load(\n","    open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\")\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### create Analyzer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:21.257949Z","iopub.status.busy":"2024-01-20T07:41:21.257563Z","iopub.status.idle":"2024-01-20T07:41:25.944228Z","shell.execute_reply":"2024-01-20T07:41:25.943032Z","shell.execute_reply.started":"2024-01-20T07:41:21.257917Z"},"trusted":true},"outputs":[],"source":["# analyzer = AnalyzerEngine()#创建文本分析器\n","configuration = {\n","    \"nlp_engine_name\": \"spacy\",\n","    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n","}\n","\n","# Create NLP engine based on configuration\n","provider = NlpEngineProvider(nlp_configuration=configuration)\n","nlp_engine = provider.create_engine()\n","\n","# create address recognizer  创建地址分析器\n","address_regex = r\"\\b\\d+\\s+\\w+(\\s+\\w+)*\\s+((st(\\.)?)|(ave(\\.)?)|(rd(\\.)?)|(blvd(\\.)?)|(ln(\\.)?)|(ct(\\.)?)|(dr(\\.)?))\\b\"\n","address_pattern = Pattern(name=\"address\", regex=address_regex, score=0.5)\n","address_recognizer = PatternRecognizer(\n","    supported_entity=\"ADDRESS_CUSTOM\", patterns=[address_pattern], context=[\"st\", \"Apt\"]\n",")\n","\n","# create address recognizer  创建邮箱分析器\n","email_regex = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n","email_pattern = Pattern(name=\"email address\", regex=email_regex, score=0.5)\n","email_recognizer = PatternRecognizer(\n","    supported_entity=\"EMAIL_CUSTOM\", patterns=[email_pattern]\n",")\n","\n","# create url recognizer  创建URL分析器\n","url_regex = r\"https?://\\S+|www\\.\\S+\"\n","url_pattern = Pattern(name=\"url\", regex=url_regex, score=0.5)\n","url_recognizer = PatternRecognizer(\n","    supported_entity=\"URL_CUSTOM\", patterns=[url_pattern]\n",")\n","\n","# create phone recognizer  创建电话分析器\n","phone_recognizer = PhoneRecognizer(\n","    context=[\n","        \"phone\",\n","        \"number\",\n","        \"telephone\",\n","        \"cell\",\n","        \"cellphone\",\n","        \"mobile\",\n","        \"call\",\n","        \"ph\",\n","        \"tel\",\n","        \"mobile\",\n","        \"Email\",\n","    ]\n",")\n","\n","\n","registry = RecognizerRegistry()\n","registry.load_predefined_recognizers()\n","registry.add_recognizer(address_recognizer)\n","registry.add_recognizer(email_recognizer)\n","registry.add_recognizer(url_recognizer)\n","registry.add_recognizer(phone_recognizer)\n","\n","\n","# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n","analyzer = AnalyzerEngine(\n","    nlp_engine=nlp_engine,\n","    supported_languages=[\"en\"],\n","    registry=registry,\n","    context_aware_enhancer=LemmaContextAwareEnhancer(\n","        context_similarity_factor=0.8, min_score_with_context_similarity=0.4\n","    ),\n",")\n","\n","\n","# remove date info in phone number recognizer  移除日期类型的电话号码\n","def is_valid_date(text):\n","    try:\n","        # Attempt to parse the text as a date\n","        parsed_date = parser.parse(text)\n","        return True\n","    except:\n","        return False"]},{"cell_type":"markdown","metadata":{},"source":["### Function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:25.946007Z","iopub.status.busy":"2024-01-20T07:41:25.945739Z","iopub.status.idle":"2024-01-20T07:41:25.95386Z","shell.execute_reply":"2024-01-20T07:41:25.953069Z","shell.execute_reply.started":"2024-01-20T07:41:25.945983Z"},"trusted":true},"outputs":[],"source":["# 对文本进行分词成下标,也就是每个词的起始位置和终止位置\n","def tokens2index(row):  # 传入一个json解析的数据\n","    tokens = row[\"tokens\"]  # 分词的数据['apple','bool','cat',……]\n","    start_ind = []\n","    end_ind = []\n","    prev_ind = 0\n","    for tok in tokens:  # 取出一个词\n","        # 比如现在的位置是30,从30开始往后找index为5,那么起始位置就是35\n","        start = prev_ind + row[\"full_text\"][prev_ind:].index(tok)\n","        end = start + len(tok)  # 起始位置+词的长度=终点位置\n","        # 储存这个词的起点和终点位置\n","        start_ind.append(start)\n","        end_ind.append(end)\n","        prev_ind = end\n","    return start_ind, end_ind  # 返回的是分词后每个词的起始位置和终点位置\n","\n","\n","# 二分查找,找到arr[index]=target\n","def find_or_next_larger(arr, target):  # arr:分词后每个词的start,target:一个实体的start\n","    left, right = 0, len(arr) - 1  # arr的最左边和最右边\n","\n","    while left <= right:\n","        mid = (left + right) // 2\n","\n","        if arr[mid] == target:\n","            return mid\n","        elif arr[mid] < target:\n","            left = mid + 1\n","        else:\n","            right = mid - 1\n","    return left\n","\n","\n","def count_trailing_whitespaces(word):\n","    # 单词的长度-单词去掉尾部空格后的长度=单词尾部的长度\n","    return len(word) - len(word.rstrip())"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:25.955218Z","iopub.status.busy":"2024-01-20T07:41:25.954935Z","iopub.status.idle":"2024-01-20T07:41:26.157296Z","shell.execute_reply":"2024-01-20T07:41:26.155642Z","shell.execute_reply.started":"2024-01-20T07:41:25.955192Z"},"trusted":true},"outputs":[],"source":["# Add URL black list 创建URL黑名单\n","black_list = [\n","    \"wikipedia\",\n","    \"coursera\",\n","    \".pdf\",\n","    \".PDF\",\n","    \"article\",\n","    \".png\",\n","    \".gov\",\n","    \".work\",\n","    \".ai\",\n","    \".firm\",\n","    \".arts\",\n","    \".store\",\n","    \".rec\",\n","    \".biz\",\n","    \".travel\",\n","]\n","white_list = [\n","    \"phone\",\n","    \"number\",\n","    \"telephone\",\n","    \"cell\",\n","    \"cellphone\",\n","    \"mobile\",\n","    \"call\",\n","    \"ph\",\n","    \"tel\",\n","    \"mobile\",\n","    \"Email\",\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:26.158911Z","iopub.status.busy":"2024-01-20T07:41:26.158587Z","iopub.status.idle":"2024-01-20T07:52:54.113034Z","shell.execute_reply":"2024-01-20T07:52:54.111889Z","shell.execute_reply.started":"2024-01-20T07:41:26.158885Z"},"trusted":true},"outputs":[],"source":["df_ = test_df  # test_df #train_df\n","PHONE_NUM, NAME_STUDENT, URL_PERSONAL, EMAIL, STREET_ADDRESS, ID_NUM, USERNAME = (\n","    [],\n","    [],\n","    [],\n","    [],\n","    [],\n","    [],\n","    [],\n",")\n","\n","preds = []\n","# 查找每个词分词后的起始位置和终点位置\n","for i in tqdm(range(len(df_)), desc=\"Processing tokens2index\"):\n","    start, end = tokens2index(df_[i])\n","    # 将每个词分词后的起始位置和终点位置加入json文件里.\n","    df_[i][\"start\"] = start\n","    df_[i][\"end\"] = end\n","\n","for i, d in tqdm(\n","    enumerate(df_), total=len(df_), desc=\"Analyzing entities\"\n","):  # 取出d=df_[i]\n","    # 传入的文本是full_text,对英文文本进行分析,需要识别的是电话号码,人,url和email这几种类型.\n","    # results:[type: PERSON, start: 22, end: 37, score: 0.85]\n","    results = analyzer.analyze(\n","        text=d[\"full_text\"],\n","        entities=[\n","            \"PHONE_NUMBER\",\n","            \"PERSON\",\n","            \"URL_CUSTOM\",  # \"IP_ADDRESS\", #\"URL\",\n","            \"EMAIL_ADDRESS\",\n","            \"EMAIL_CUSTOM\",\n","            \"ADDRESS_CUSTOM\",\n","            \"US_SSN\",\n","            \"US_ITIN\",\n","            \"US_PASSPORT\",\n","            \"US_BANK_NUMBER\",\n","            \"USERNAME\",\n","        ],\n","        language=\"en\",\n","        #                            score_threshold=0.04,\n","    )\n","    pre_preds = []\n","    for r in results:  # 遍历找到过的每个实体,r:[type: PERSON, start: 22, end: 37, score: 0.85]\n","        # 就是第s个词就是某个实体的开始\n","        s = find_or_next_larger(d[\"start\"], r.start)  # d['start'][s]=r.start\n","        end = r.end  # 实体终点\n","        word = d[\"full_text\"][r.start : r.end]  # 文本里找单词\n","        end = end - count_trailing_whitespaces(word)  # end减去尾部的空格就是单词自身尾部的下标\n","        temp_preds = [s]  # 实体单词的集合从第s个单词开始,然后连续几个单词?\n","        try:\n","            # 实体可能不是一个单词,分词的下一个单词如果还没有到达实体的尾部,就把下一个单词加上\n","            while d[\"end\"][s + 1] <= end:\n","                temp_preds.append(s + 1)\n","                s += 1\n","        except:\n","            pass\n","\n","        # 找出来的实体是什么,我们就给它打对应的标签\n","        tmp = False\n","\n","        if r.entity_type == \"USERNAME\":\n","            label = \"USERNAME\"\n","            USERNAME.append(d[\"full_text\"][r.start : r.end])\n","\n","        if r.entity_type == \"PHONE_NUMBER\":\n","            # 检查是不是日期类型\n","            if is_valid_date(word):\n","                continue\n","            for w in white_list:\n","                if (\n","                    w\n","                    in d[\"full_text\"][\n","                        max(r.start - 50, 0) : min(r.end + 50, len(d[\"full_text\"]))\n","                    ]\n","                ):\n","                    tmp = False\n","                    break\n","                else:\n","                    tmp = True\n","\n","            label = \"PHONE_NUM\"\n","            PHONE_NUM.append(d[\"full_text\"][r.start : r.end])\n","\n","        if r.entity_type == \"PERSON\":\n","            label = \"NAME_STUDENT\"\n","            NAME_STUDENT.append(d[\"full_text\"][r.start : r.end])\n","\n","        if r.entity_type == \"ADDRESS_CUSTOM\":\n","            label = \"STREET_ADDRESS\"\n","            STREET_ADDRESS.append(d[\"full_text\"][r.start : r.end])\n","\n","        if (\n","            r.entity_type == \"US_SSN\"\n","            or r.entity_type == \"US_ITIN\"\n","            or r.entity_type == \"US_PASSPORT\"\n","            or r.entity_type == \"US_BANK_NUMBER\"\n","        ):\n","            label = \"ID_NUM\"\n","            ID_NUM.append(d[\"full_text\"][r.start : r.end])\n","\n","        if r.entity_type == \"EMAIL_ADDRESS\" or r.entity_type == \"EMAIL_CUSTOM\":\n","            label = \"EMAIL\"\n","            EMAIL.append(d[\"full_text\"][r.start : r.end])\n","\n","        if (\n","            r.entity_type == \"URL_CUSTOM\"\n","        ):  # or r.entity_type == 'IP_ADDRESS' or \"http\" in word:\n","            # 去除掉黑名单里的标签\n","            for w in black_list:\n","                if w in word:\n","                    tmp = True\n","                    break\n","\n","            label = \"URL_PERSONAL\"\n","            URL_PERSONAL.append(d[\"full_text\"][r.start : r.end])\n","\n","        if tmp:\n","            continue\n","\n","        # 取出实体中的一个分词的下标\n","        for p in temp_preds:\n","            if len(pre_preds) > 0:  # 第2次及以后经过这里.\n","                \"\"\"\n","                新开始一个r的时候,pre_preds[-1]['rlabel']还是上一个实体的r.entity_type\n","                此时也许会不等于这个实体的r.entity_type,换句话说,第一个等号就是还在同一个实体里.\n","                p - pre_preds[-1]['token']==1就是连续的意思\n","                \"\"\"\n","                if pre_preds[-1][\"rlabel\"] == r.entity_type and (\n","                    p - pre_preds[-1][\"token\"] == 1\n","                ):\n","                    label_f = \"I-\" + label  # 实体的中间位置\n","                else:\n","                    label_f = \"B-\" + label  # 否则就是下一个实体的开始\n","            else:  # 第一个label是起始位置,故标记为‘B-’\n","                label_f = \"B-\" + label\n","            # 保存document,从第p个单词开始,标签为label_f\n","            pre_preds.append(\n","                (\n","                    {\n","                        \"document\": d[\"document\"],\n","                        \"token\": p,\n","                        \"label\": label_f,\n","                        \"rlabel\": r.entity_type,  # 实体的类型\n","                    }\n","                )\n","            )\n","    preds.extend(pre_preds)  # 遍历完这个数据之后,将所有找到的实体做汇总"]},{"cell_type":"markdown","metadata":{},"source":["### Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:52:54.114603Z","iopub.status.busy":"2024-01-20T07:52:54.114315Z","iopub.status.idle":"2024-01-20T07:52:54.19626Z","shell.execute_reply":"2024-01-20T07:52:54.1952Z","shell.execute_reply.started":"2024-01-20T07:52:54.114574Z"},"trusted":true},"outputs":[],"source":["# 得到预测结果后,最后一行r.entity_type不要,reset_index\n","submission = pd.DataFrame(preds).iloc[:, :-1].reset_index()\n","# index变成row_id,剩下3列就是submission的列名\n","submission.columns = [\"row_id\", \"document\", \"token\", \"label\"]\n","# 保存csv文件\n","submission.to_csv(\"submission.csv\", index=False)\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"sourceId":159367535,"sourceType":"kernelVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}

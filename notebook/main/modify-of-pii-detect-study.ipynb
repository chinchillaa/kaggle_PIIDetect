{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### This notebook is modified from <a href=\"https://www.kaggle.com/code/pjmathematician/pii-eda-presidio-baseline\">PII EDA Presidio Baseline</a> and <a href=\"https://www.kaggle.com/code/yunsuxiaozi/pii-detect-study-notebook\">PII detect study notebook</a>. "]},{"cell_type":"markdown","metadata":{},"source":["## Modifications \n","\n","#### I add my own address_recognizer and email_recognizer, URL_recognizer, and add a black list to filter potential public urls and date checker to filter noisy phone numbers. I also added Chinese note for my modifications."]},{"cell_type":"markdown","metadata":{},"source":["### Install presidio"]},{"cell_type":"code","execution_count":28,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-20T07:40:57.487624Z","iopub.status.busy":"2024-01-20T07:40:57.486577Z","iopub.status.idle":"2024-01-20T07:41:13.824483Z","shell.execute_reply":"2024-01-20T07:41:13.823011Z","shell.execute_reply.started":"2024-01-20T07:40:57.48757Z"},"trusted":true},"outputs":[],"source":["# !pip install -U -q presidio_analyzer --no-index --find-links=/Users/0ne/Programming/Kaggle/PIIDetect/data/presidio"]},{"cell_type":"markdown","metadata":{},"source":["### Import  necessary libraries"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:13.826839Z","iopub.status.busy":"2024-01-20T07:41:13.826484Z","iopub.status.idle":"2024-01-20T07:41:18.852087Z","shell.execute_reply":"2024-01-20T07:41:18.850875Z","shell.execute_reply.started":"2024-01-20T07:41:13.826808Z"},"trusted":true},"outputs":[],"source":["import json\n","import pandas as pd\n","from tqdm import tqdm\n","from typing import List\n","import pprint\n","import re\n","from pathlib import Path\n","\n","# Presidio\n","from presidio_analyzer import AnalyzerEngine\n","from presidio_analyzer.nlp_engine import NlpEngineProvider\n","\n","from presidio_analyzer import (\n","    AnalyzerEngine,\n","    PatternRecognizer,\n","    EntityRecognizer,\n","    Pattern,\n","    RecognizerResult,\n",")\n","from presidio_analyzer.recognizer_registry import RecognizerRegistry\n","from presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\n","from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n","\n","from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n","from presidio_analyzer.predefined_recognizers import PhoneRecognizer\n","from dateutil import parser"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/plain":["'/opt/anaconda3/envs/dl_env/lib/python3.8/site-packages/pandas/__init__.py'"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["pd.__file__"]},{"cell_type":"markdown","metadata":{},"source":["# Environment setting functions"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def _is_in_kaggle() -> bool:\n","    \"\"\"Whether the current environment is in Kaggle.\"\"\"\n","    return str(_dh[0]) == \"/kaggle/working\"\n","\n","\n","def _is_in_colab() -> bool:\n","    \"\"\"Whether the current environment is in Colab.\"\"\"\n","    return \"google.colab\" in str(get_ipython())"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def _set_environment() -> str:\n","    \"\"\"Set and print the environment.\"\"\"\n","    if _is_in_kaggle():\n","        my_env = \"in kaggle\"\n","    elif _is_in_colab():\n","        my_env = \"in colab\"\n","    else:\n","        my_env = \"in local\"\n","    print(f\"I'm in {my_env}\")\n","\n","    return my_env"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["def set_base_directory() -> str:\n","    \"\"\"Set and print the base directory.\"\"\"\n","    my_env = _set_environment()\n","    if my_env == \"in kaggle\":\n","        base_dir = Path(\"/kaggle/input\")\n","    elif my_env == \"in colab\":\n","        base_dir = Path(\"/content/data\")\n","    else:\n","        base_dir = Path(\"../../data\")\n","    print(f\"base_dir:{base_dir}\")\n","\n","    return base_dir"]},{"cell_type":"markdown","metadata":{},"source":["### Import dataset"]},{"cell_type":"code","execution_count":35,"metadata":{"notebookRunGroups":{"groupValue":"1"}},"outputs":[{"name":"stdout","output_type":"stream","text":["I'm in in local\n","base_dir:../../data\n"]}],"source":["base_dir = set_base_directory()\n","piidetect_dir = base_dir / \"pii-detection-removal-from-educational-data\"\n","print(f\"piidetect_dir:{piidetect_dir}\")"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["len(train_df):6807,train_df[0].keys():dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])\n","--------------------------------------------------\n","labels:{'B-URL_PERSONAL', 'B-STREET_ADDRESS', 'I-PHONE_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'I-URL_PERSONAL', 'B-ID_NUM', 'O', 'I-STREET_ADDRESS', 'I-NAME_STUDENT', 'B-EMAIL', 'B-USERNAME', 'I-ID_NUM'}\n"]}],"source":["train_df = json.load(open(piidetect_dir / \"train.json\"))\n","print(f\"len(train_df):{len(train_df)},train_df[0].keys():{train_df[0].keys()}\")\n","print(\"-\" * 50)\n","labels = set()\n","for i in range(len(train_df)):\n","    labels.update(train_df[i][\"labels\"])\n","print(f\"labels:{labels}\")\n","test_df = json.load(open(piidetect_dir / \"test.json\"))"]},{"cell_type":"markdown","metadata":{},"source":["### create Analyzer"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:21.257949Z","iopub.status.busy":"2024-01-20T07:41:21.257563Z","iopub.status.idle":"2024-01-20T07:41:25.944228Z","shell.execute_reply":"2024-01-20T07:41:25.943032Z","shell.execute_reply.started":"2024-01-20T07:41:21.257917Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting en-core-web-lg==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from en-core-web-lg==3.7.1) (3.7.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.2)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.5.3)\n","Requirement already satisfied: jinja2 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.2)\n","Requirement already satisfied: setuptools in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (68.2.2)\n","Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.0)\n","Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.24.3)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.14.6)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2023.11.17)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/dl_env/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.3)\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-3.7.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_lg')\n"]}],"source":["# analyzer = AnalyzerEngine()\n","configuration = {\n","    \"nlp_engine_name\": \"spacy\",\n","    \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n","}\n","\n","# Create NLP engine based on configuration\n","provider = NlpEngineProvider(nlp_configuration=configuration)\n","nlp_engine = provider.create_engine()\n","\n","# create address recognizer\n","address_regex = r\"\\b\\d+\\s+\\w+(\\s+\\w+)*\\s+((st(\\.)?)|(ave(\\.)?)|(rd(\\.)?)|(blvd(\\.)?)|(ln(\\.)?)|(ct(\\.)?)|(dr(\\.)?))\\b\"\n","address_pattern = Pattern(name=\"address\", regex=address_regex, score=0.5)\n","address_recognizer = PatternRecognizer(\n","    supported_entity=\"ADDRESS_CUSTOM\", patterns=[address_pattern], context=[\"st\", \"Apt\"]\n",")\n","\n","# create address recognizer\n","email_regex = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n","email_pattern = Pattern(name=\"email address\", regex=email_regex, score=0.5)\n","email_recognizer = PatternRecognizer(\n","    supported_entity=\"EMAIL_CUSTOM\", patterns=[email_pattern]\n",")\n","\n","# create url recognizer\n","url_regex = r\"https?://\\S+|www\\.\\S+\"\n","url_pattern = Pattern(name=\"url\", regex=url_regex, score=0.5)\n","url_recognizer = PatternRecognizer(\n","    supported_entity=\"URL_CUSTOM\", patterns=[url_pattern]\n",")\n","\n","# create phone recognizer\n","phone_recognizer = PhoneRecognizer(\n","    context=[\n","        \"phone\",\n","        \"number\",\n","        \"telephone\",\n","        \"cell\",\n","        \"cellphone\",\n","        \"mobile\",\n","        \"call\",\n","        \"ph\",\n","        \"tel\",\n","        \"mobile\",\n","        \"Email\",\n","    ]\n",")\n","\n","\n","registry = RecognizerRegistry()\n","registry.load_predefined_recognizers()\n","registry.add_recognizer(address_recognizer)\n","registry.add_recognizer(email_recognizer)\n","registry.add_recognizer(url_recognizer)\n","registry.add_recognizer(phone_recognizer)\n","\n","\n","# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n","analyzer = AnalyzerEngine(\n","    nlp_engine=nlp_engine,\n","    supported_languages=[\"en\"],\n","    registry=registry,\n","    context_aware_enhancer=LemmaContextAwareEnhancer(\n","        context_similarity_factor=0.8, min_score_with_context_similarity=0.4\n","    ),\n",")\n","\n","\n","# remove date info in phone number recognizer\n","def is_valid_date(text):\n","    try:\n","        # Attempt to parse the text as a date\n","        parsed_date = parser.parse(text)\n","        return True\n","    except:\n","        return False"]},{"cell_type":"markdown","metadata":{},"source":["### Function"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:25.946007Z","iopub.status.busy":"2024-01-20T07:41:25.945739Z","iopub.status.idle":"2024-01-20T07:41:25.95386Z","shell.execute_reply":"2024-01-20T07:41:25.953069Z","shell.execute_reply.started":"2024-01-20T07:41:25.945983Z"},"trusted":true},"outputs":[],"source":["def tokens2index(row):\n","    tokens = row[\"tokens\"]\n","    start_ind = []\n","    end_ind = []\n","    prev_ind = 0\n","    for tok in tokens:\n","        start = prev_ind + row[\"full_text\"][prev_ind:].index(tok)\n","        end = start + len(tok)\n","        start_ind.append(start)\n","        end_ind.append(end)\n","        prev_ind = end\n","    return start_ind, end_ind\n","\n","\n","def find_or_next_larger(arr, target):\n","    left, right = 0, len(arr) - 1\n","\n","    while left <= right:\n","        mid = (left + right) // 2\n","\n","        if arr[mid] == target:\n","            return mid\n","        elif arr[mid] < target:\n","            left = mid + 1\n","        else:\n","            right = mid - 1\n","    return left\n","\n","\n","def count_trailing_whitespaces(word):\n","    return len(word) - len(word.rstrip())"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:25.955218Z","iopub.status.busy":"2024-01-20T07:41:25.954935Z","iopub.status.idle":"2024-01-20T07:41:26.157296Z","shell.execute_reply":"2024-01-20T07:41:26.155642Z","shell.execute_reply.started":"2024-01-20T07:41:25.955192Z"},"trusted":true},"outputs":[],"source":["black_list = [\n","    \"wikipedia\",\n","    \"coursera\",\n","    \".pdf\",\n","    \".PDF\",\n","    \"article\",\n","    \".png\",\n","    \".gov\",\n","    \".work\",\n","    \".ai\",\n","    \".firm\",\n","    \".arts\",\n","    \".store\",\n","    \".rec\",\n","    \".biz\",\n","    \".travel\",\n","]\n","white_list = [\n","    \"phone\",\n","    \"number\",\n","    \"telephone\",\n","    \"cell\",\n","    \"cellphone\",\n","    \"mobile\",\n","    \"call\",\n","    \"ph\",\n","    \"tel\",\n","    \"mobile\",\n","    \"Email\",\n","]"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:41:26.158911Z","iopub.status.busy":"2024-01-20T07:41:26.158587Z","iopub.status.idle":"2024-01-20T07:52:54.113034Z","shell.execute_reply":"2024-01-20T07:52:54.111889Z","shell.execute_reply.started":"2024-01-20T07:41:26.158885Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing tokens2index: 100%|██████████| 10/10 [00:00<00:00, 605.69it/s]\n","Analyzing entities: 100%|██████████| 10/10 [00:02<00:00,  4.02it/s]\n"]}],"source":["df_ = test_df  # test_df #train_df\n","PHONE_NUM, NAME_STUDENT, URL_PERSONAL, EMAIL, STREET_ADDRESS, ID_NUM, USERNAME = (\n","    [],\n","    [],\n","    [],\n","    [],\n","    [],\n","    [],\n","    [],\n",")\n","\n","preds = []\n","for i in tqdm(range(len(df_)), desc=\"Processing tokens2index\"):\n","    start, end = tokens2index(df_[i])\n","    df_[i][\"start\"] = start\n","    df_[i][\"end\"] = end\n","\n","for i, d in tqdm(enumerate(df_), total=len(df_), desc=\"Analyzing entities\"):\n","    results = analyzer.analyze(\n","        text=d[\"full_text\"],\n","        entities=[\n","            \"PHONE_NUMBER\",\n","            \"PERSON\",\n","            \"URL_CUSTOM\",  # \"IP_ADDRESS\", #\"URL\",\n","            \"EMAIL_ADDRESS\",\n","            \"EMAIL_CUSTOM\",\n","            \"ADDRESS_CUSTOM\",\n","            \"US_SSN\",\n","            \"US_ITIN\",\n","            \"US_PASSPORT\",\n","            \"US_BANK_NUMBER\",\n","            \"USERNAME\",\n","        ],\n","        language=\"en\",\n","        #                            score_threshold=0.04,\n","    )\n","    pre_preds = []\n","    for r in results:\n","        s = find_or_next_larger(d[\"start\"], r.start)  # d['start'][s]=r.start\n","        end = r.end  # 实体终点\n","        word = d[\"full_text\"][r.start : r.end]  # 文本里找单词\n","        end = end - count_trailing_whitespaces(word)  # end减去尾部的空格就是单词自身尾部的下标\n","        temp_preds = [s]  # 实体单词的集合从第s个单词开始,然后连续几个单词?\n","        try:\n","            # 实体可能不是一个单词,分词的下一个单词如果还没有到达实体的尾部,就把下一个单词加上\n","            while d[\"end\"][s + 1] <= end:\n","                temp_preds.append(s + 1)\n","                s += 1\n","        except:\n","            pass\n","\n","        # 找出来的实体是什么,我们就给它打对应的标签\n","        tmp = False\n","\n","        if r.entity_type == \"USERNAME\":\n","            label = \"USERNAME\"\n","            USERNAME.append(d[\"full_text\"][r.start : r.end])\n","\n","        if r.entity_type == \"PHONE_NUMBER\":\n","            # 检查是不是日期类型\n","            if is_valid_date(word):\n","                continue\n","            for w in white_list:\n","                if (\n","                    w\n","                    in d[\"full_text\"][\n","                        max(r.start - 50, 0) : min(r.end + 50, len(d[\"full_text\"]))\n","                    ]\n","                ):\n","                    tmp = False\n","                    break\n","                else:\n","                    tmp = True\n","\n","            label = \"PHONE_NUM\"\n","            PHONE_NUM.append(d[\"full_text\"][r.start : r.end])\n","\n","        if r.entity_type == \"PERSON\":\n","            label = \"NAME_STUDENT\"\n","            NAME_STUDENT.append(d[\"full_text\"][r.start : r.end])\n","\n","        if r.entity_type == \"ADDRESS_CUSTOM\":\n","            label = \"STREET_ADDRESS\"\n","            STREET_ADDRESS.append(d[\"full_text\"][r.start : r.end])\n","\n","        if (\n","            r.entity_type == \"US_SSN\"\n","            or r.entity_type == \"US_ITIN\"\n","            or r.entity_type == \"US_PASSPORT\"\n","            or r.entity_type == \"US_BANK_NUMBER\"\n","        ):\n","            label = \"ID_NUM\"\n","            ID_NUM.append(d[\"full_text\"][r.start : r.end])\n","\n","        if r.entity_type == \"EMAIL_ADDRESS\" or r.entity_type == \"EMAIL_CUSTOM\":\n","            label = \"EMAIL\"\n","            EMAIL.append(d[\"full_text\"][r.start : r.end])\n","\n","        if (\n","            r.entity_type == \"URL_CUSTOM\"\n","        ):  # or r.entity_type == 'IP_ADDRESS' or \"http\" in word:\n","            # 去除掉黑名单里的标签\n","            for w in black_list:\n","                if w in word:\n","                    tmp = True\n","                    break\n","\n","            label = \"URL_PERSONAL\"\n","            URL_PERSONAL.append(d[\"full_text\"][r.start : r.end])\n","\n","        if tmp:\n","            continue\n","\n","        # 取出实体中的一个分词的下标\n","        for p in temp_preds:\n","            if len(pre_preds) > 0:  # 第2次及以后经过这里.\n","                \"\"\"\n","                新开始一个r的时候,pre_preds[-1]['rlabel']还是上一个实体的r.entity_type\n","                此时也许会不等于这个实体的r.entity_type,换句话说,第一个等号就是还在同一个实体里.\n","                p - pre_preds[-1]['token']==1就是连续的意思\n","                \"\"\"\n","                if pre_preds[-1][\"rlabel\"] == r.entity_type and (\n","                    p - pre_preds[-1][\"token\"] == 1\n","                ):\n","                    label_f = \"I-\" + label  # 实体的中间位置\n","                else:\n","                    label_f = \"B-\" + label  # 否则就是下一个实体的开始\n","            else:  # 第一个label是起始位置,故标记为‘B-’\n","                label_f = \"B-\" + label\n","            # 保存document,从第p个单词开始,标签为label_f\n","            pre_preds.append(\n","                (\n","                    {\n","                        \"document\": d[\"document\"],\n","                        \"token\": p,\n","                        \"label\": label_f,\n","                        \"rlabel\": r.entity_type,  # 实体的类型\n","                    }\n","                )\n","            )\n","    preds.extend(pre_preds)  # 遍历完这个数据之后,将所有找到的实体做汇总"]},{"cell_type":"markdown","metadata":{},"source":["### Submission"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2024-01-20T07:52:54.114603Z","iopub.status.busy":"2024-01-20T07:52:54.114315Z","iopub.status.idle":"2024-01-20T07:52:54.19626Z","shell.execute_reply":"2024-01-20T07:52:54.1952Z","shell.execute_reply.started":"2024-01-20T07:52:54.114574Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>document</th>\n","      <th>token</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>9</td>\n","      <td>B-NAME_STUDENT</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>10</td>\n","      <td>I-NAME_STUDENT</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>7</td>\n","      <td>52</td>\n","      <td>B-NAME_STUDENT</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>53</td>\n","      <td>I-NAME_STUDENT</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>55</td>\n","      <td>B-NAME_STUDENT</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   row_id  document  token           label\n","0       0         7      9  B-NAME_STUDENT\n","1       1         7     10  I-NAME_STUDENT\n","2       2         7     52  B-NAME_STUDENT\n","3       3         7     53  I-NAME_STUDENT\n","4       4         7     55  B-NAME_STUDENT"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["# 得到预测结果后,最后一行r.entity_type不要,reset_index\n","submission = pd.DataFrame(preds).iloc[:, :-1].reset_index()\n","# index变成row_id,剩下3列就是submission的列名\n","submission.columns = [\"row_id\", \"document\", \"token\", \"label\"]\n","# 保存csv文件\n","submission.to_csv(\"submission.csv\", index=False)\n","submission.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"},{"sourceId":159367535,"sourceType":"kernelVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
